# American Sign Language Recognition using Deep Neural Network
## 1 - Introduction
ASL Recognition with Deep Learning, convolutional neural network to classify images of letters from American Sign Language.

American Sign Language (ASL) is the primary language used by many deaf individuals in North America, and it is also used by hard-of-hearing and hearing individuals. The language is as rich as spoken languages and employs signs made with the hand, along with facial gestures and bodily postures.

A lot of recent progress has been made towards developing computer vision systems that translate sign language to spoken language. This technology often relies on complex neural network architectures that can detect subtle patterns in streaming video. However, as a first step, towards understanding how to build a translation system, we can reduce the size of the problem by translating individual letters, instead of sentences.

## 2 - Approach
In this notebook, I trained a convolutional neural network to classify images of American Sign Language (ASL) letters. After loading, examining, and preprocessing the data, I trained the network and tested its performance.

Tasks:
1. American Sign Language (ASL)
2. Visualize the training data
3. Examine the dataset
4. One-hot encode the data
5. Define the model
6. Compile the model
7. Train the model
8. Test the model
9. Visualize mistakes

## 3 - Results
A functional real time vision based American Sign Language recognition for the deaf and dumb people have been developed for english alphabets. We achieved final accuracy of 99.86% on our dataset. This way we are able to detect almost all the symbols provided that they are shown properly, given there is no noise in the background and lighting is adequate